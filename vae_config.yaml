apache_params:
  blade_chord_m: .533
  rotor_diameter_m: 14.63
  wheel_height_m: .76196
  phase_center_offset_m: .4953
  rotor_velocity_rad_s: 29.7404
  rotor_pitch_max: -10
  rotor_pitch_min: 30
  dismount_slant_range_min: 500
  dismount_slant_range_max: 15000
  vehicle_slant_range_min: 500
  vehicle_slant_range_max: 25000
  alt_min: 30.48
  alt_max: 1524.
  az_min_bw: .866
  el_min_bw: 8.

settings:
  cpi_len: 32
  batch_sz: 128
  fc: 10000000000.
  bandwidth: 400000000.
  stft_win_sz: 256
  plp: .5
  n_ants: 2
  fft_len: 8192

generate_data_settings:
  iterations: 1
  use_local_storage: True
  save_files: True
  overwrite_files: True  # Don't set this to False without cleaning the folders first
  local_path: ./data
  obj_path: /home/jeff/repo/apache/data/target_meshes
  run_target: False
  run_clutter: True
  seq_min_length: 8
  seq_max_length: 16
  clutter_iterations: 5
  supersamples: 0
  n_az_samples: 16
  n_el_samples: 8
  num_bounces: 1
  fc: 9600000000.
  rx_gain: 40  # dB
  tx_gain: 40  # dB
  rec_gain: 100  # dB
  fs: 2000000000.
  ant_transmit_power: 100  # Watts
  nsam: 4600
  nstreams: 1
  num_sample_points_power: 17  # Power of two for samples (2**n)


    
# This is for setting up training of target embedding
target_exp_params:
  training_params:
    lr: 0.00001
    weight_decay: 0.01
    scheduler_gamma: 0.9999  # Exponential decay
    eta_min: .000000001  # Cosine annealing minimum - 1e-9
    betas: [ .9, .99 ]
    log_epoch: 1
    save_model: True
    transform_data: False
    warm_start: False
    loss_landscape: False
    max_epochs: 20000
    patience: 10
    angle_samples: 128
  logging_params:
    output_images: False
    model_name: target_encoder
    is_tuning: False
    is_training: True
    log_dir: ./logs
    weights_path: ./model
  dataset_params:
    data_path: ./data/target_tensors
    train_batch_size: 10
    val_batch_size: 10
    num_workers: 0
    pin_memory: False
    split: .8
    single_example: False
    # These values are taken from a sampling of data
    mu: [.1050, .1049, .1617, .1620]
    var: [38.7205, 38.7190, 38.8960, 38.8978]
    noise_level: 0.
  model_params:
    fft_len: 8192
    range_samples: 2
    latent_dim: 1000
    channel_sz: 256
    nonlinearity: selu
    temperature: 1.
    levels: 1
    # These values are taken from a sampling of data, should be the same as the dataset params
    mu: [.1050, .1049, .1617, .1620]
    var: [38.7205, 38.7190, 38.8960, 38.8978]

# For training of wavemodel
wave_exp_params:
  training_params:
    lr: 0.0001
    weight_decay: 0.0
    scheduler_gamma: 0.999
    swa_start: .6
    warm_start: False
    betas: [.9, .99]
    log_epoch: 100
    accumulation_steps: 8
    is_tuning: False
    save_model: False
    loss_landscape: False
    gradient_flow: False
    max_epochs: 1200
    patience: 500
    distributed: False
  model_params:
    n_ants: 2
    fft_len: 8192
    fs: 2000000000.
    fc: 9600000000.
    transformer_passes: 1
    encoder_start_channel_sz: 2
    embedding_concatenation_channels: 128
    n_skip_layers: 3
    clutter_target_channels: 128
    flowthrough_channels: 128
    wave_decoder_channels: 128
    target_channels: 128
    waveform_channels: 256
    target_latent_size: 1000
    n_fourier_modes: 64
    bandwidth: 400000000.
    nonlinearity: selu
  logging_params:
    log_dir: ./logs
    weights_path: ./model
    model_name: wave_model
  dataset_params:
    data_path: ./data
    train_batch_size: 1
    val_batch_size: 1
    num_workers: 0
    pin_memory: False
    split: .7
    single_example: False
    # These values are taken from a sampling of data
    mu: 199926611.3141519
    var: 52500339516.69853
    noise_level: 0.
    collate: True

train_params:
  log_dir: ./logs
