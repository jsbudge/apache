apache_params:
  blade_chord_m: .533
  rotor_diameter_m: 14.63
  wheel_height_m: .76196
  phase_center_offset_m: .4953
  rotor_velocity_rad_s: 29.7404
  rotor_pitch_max: -10
  rotor_pitch_min: 30
  dismount_slant_range_min: 500
  dismount_slant_range_max: 15000
  vehicle_slant_range_min: 500
  vehicle_slant_range_max: 25000
  alt_min: 30.48
  alt_max: 1524.
  az_min_bw: .866
  el_min_bw: 8.

settings:
  cpi_len: 32
  batch_sz: 128
  fc: 10000000000.
  bandwidth: 400000000.
  stft_win_sz: 256
  plp: .5
  n_ants: 2
  fft_len: 8192

generate_data_settings:
  iterations: 10
  use_local_storage: True
  save_files: True
  local_path: ./data
  obj_path: /home/jeff/Documents/target_meshes
  save_as_target: True
  save_as_clutter: False

# This is for setting up training of autoencoder
exp_params:
  LR: 0.001
  weight_decay: 0.01
  scheduler_gamma: 0.5
  betas: [.07, .9]
  step_size: 1
  swa_start: .5
  log_epoch: 50
  save_model: True
  transform_data: True
  warm_start: True
  loss_landscape: False
  output_images: False
  model_type: Encoder
  patience: 150
  is_tuning: False
  max_epochs: 150
  exp_name: train-latent612-channel64
  init_task: False
  dataset_params:
    data_path: ./data
    train_batch_size: 4
    val_batch_size: 4
    pin_memory: False
    split: .7
    single_example: False
    # These values are taken from a sampling of data
    mu: 199926611.3141519
    var: 52500339516.69853
    noise_level: 0.
  model_params:
    in_channels: 2
    latent_dim: 768
    channel_sz: 8
    
# This is for setting up training of target autoencoder
target_exp_params:
  LR: 0.0000014
  weight_decay: 0.0
  scheduler_gamma: 0.7
  betas: [ .9, .99 ]
  step_size: 1
  swa_start: .7
  log_epoch: 10
  save_model: False
  transform_data: False
  warm_start: True
  loss_landscape: False
  output_images: False
  model_type: TargetEncoder
  patience: 550
  is_tuning: False
  max_epochs: 1000
  init_task: False
  exp_name: train-targetencoder-lka-attention-normalized
  is_training: True
  dataset_params:
    data_path: ./data/target_tensors
    train_batch_size: 28
    val_batch_size: 28
    num_workers: 0
    pin_memory: False
    split: .7
    single_example: False
    # These values are taken from a sampling of data
    mu: 0.13710163081625043
    var: 4014.6355947947404
    noise_level: 0.
  model_params:
    in_channels: 2
    latent_dim: 612
    channel_sz: 32
    # These values are taken from a sampling of data, should be the same as the dataset params
    mu: 0.13710163081625043
    var: 4014.6355947947404

# For training of wavemodel
wave_exp_params:
  LR: 0.001
  weight_decay: 0.5
  scheduler_gamma: 0.5
  swa_start: .6
  warm_start: False
  betas: [.07, .09]
  step_size: 1
  log_epoch: 100
  is_tuning: False
  save_model: True
  loss_landscape: False
  patience: 5
  channel_sz: 128
  bandwidth: 400000000.
  max_epochs: 500
  init_task: False
  exp_name: wavemodel-6x6-goodsidelobes
  dataset_params:
    data_path: ./data
    train_batch_size: 2
    val_batch_size: 2
    num_workers: 0
    pin_memory: False
    split: .7
    single_example: False
    # These values are taken from a sampling of data
    mu: 199926611.3141519
    var: 52500339516.69853
    noise_level: 0.
    collate: True

# For training of rcs model
rcs_exp_params:
  LR: 0.000000001
  weight_decay: 0.08
  scheduler_gamma: 0.95
  kld_weight: .39
  log_epoch: 10
  is_tuning: False
  save_model: False
  loss_landscape: False
  output_images: True
  patience: 15

train_params:
  log_dir: ./logs
