apache_params:
  blade_chord_m: .533
  rotor_diameter_m: 14.63
  wheel_height_m: .76196
  phase_center_offset_m: .4953
  rotor_velocity_rad_s: 29.7404
  rotor_pitch_max: -10
  rotor_pitch_min: 30
  dismount_slant_range_min: 500
  dismount_slant_range_max: 15000
  vehicle_slant_range_min: 500
  vehicle_slant_range_max: 25000
  alt_min: 30.48
  alt_max: 1524.
  az_min_bw: .866
  el_min_bw: 8.

settings:
  cpi_len: 32
  batch_sz: 128
  fc: 10000000000.
  bandwidth: 400000000.
  stft_win_sz: 256
  plp: .5
  n_ants: 2
  fft_len: 8192
  save_as_target: True

generate_data_settings:
  iterations: 500
  run_clutter: True
  run_targets: False
  use_local_storage: True
  local_path: ./data
  obj_path: /home/jeff/Documents/target_meshes

# This is for setting up training of autoencoder
exp_params:
  LR: 0.001
  weight_decay: 0.0
  scheduler_gamma: 0.1
  betas: [.07, .9]
  step_size: 1
  swa_start: .5
  log_epoch: 10
  save_model: True
  transform_data: False
  warm_start: True
  loss_landscape: False
  output_images: False
  model_type: Encoder
  patience: 150
  is_tuning: False
  max_epochs: 500
  exp_name: train-latent512tanh-64chan
  init_task: True
  dataset_params:
    data_path: ./data
    train_batch_size: 32
    val_batch_size: 32
    pin_memory: False
    split: .7
    single_example: False
    # These values are taken from a sampling of data
    mu: 274662906.04691434
    var: 49527137064.739006
    noise_level: 0.
  model_params:
    in_channels: 2
    latent_dim: 512
    channel_sz: 64
    
# This is for setting up training of target autoencoder
target_exp_params:
  LR: 0.001
  weight_decay: 0.05
  scheduler_gamma: 0.7
  betas: [ .07, .9 ]
  step_size: 2
  swa_start: .7
  log_epoch: 10
  save_model: True
  transform_data: True
  warm_start: False
  loss_landscape: False
  output_images: False
  model_type: TargetEncoder
  patience: 250
  is_tuning: False
  max_epochs: 15000
  init_task: True
  exp_name: train-targetencoder-lka-attention
  dataset_params:
    data_path: ./data
    train_batch_size: 32
    val_batch_size: 32
    num_workers: 0
    pin_memory: False
    split: .7
    single_example: False
    # These values are taken from a sampling of data
    mu: 0.011546927006842008
    var: 4.939321435461906
    noise_level: 0.
  model_params:
    in_channels: 2
    latent_dim: 1024
    channel_sz: 32

# For training of wavemodel
wave_exp_params:
  LR: 0.000001
  weight_decay: 0.6
  scheduler_gamma: 0.5
  swa_start: .6
  warm_start: False
  betas: [.9, .99]
  step_size: 1
  log_epoch: 10
  is_tuning: False
  save_model: True
  loss_landscape: False
  patience: 350
  channel_sz: 64
  bandwidth: 400000000.
  max_epochs: 10
  init_task: True
  exp_name: wavemodel-double5x5-newloss-targettransform
  dataset_params:
    data_path: ./data
    train_batch_size: 64
    val_batch_size: 64
    num_workers: 0
    pin_memory: False
    split: .7
    single_example: False
    # These values are taken from a sampling of data
    mu: 274662906.04691434
    var: 49527137064.739006
    noise_level: 0.
    collate: True

# For training of rcs model
rcs_exp_params:
  LR: 0.000000001
  weight_decay: 0.08
  scheduler_gamma: 0.95
  kld_weight: .39
  log_epoch: 10
  is_tuning: False
  save_model: False
  loss_landscape: False
  output_images: True
  patience: 15

train_params:
  log_dir: ./logs
