apache_params:
  blade_chord_m: .533
  rotor_diameter_m: 14.63
  wheel_height_m: .76196
  phase_center_offset_m: .4953
  rotor_velocity_rad_s: 29.7404
  rotor_pitch_max: -10
  rotor_pitch_min: 30
  dismount_slant_range_min: 500
  dismount_slant_range_max: 15000
  vehicle_slant_range_min: 500
  vehicle_slant_range_max: 25000
  alt_min: 30.48
  alt_max: 1524.
  az_min_bw: .866
  el_min_bw: 8.

settings:
  cpi_len: 32
  batch_sz: 128
  fc: 10000000000.
  bandwidth: 400000000.
  stft_win_sz: 256
  plp: .5
  n_ants: 2
  fft_len: 8192

generate_data_settings:
  iterations: 10
  use_local_storage: True
  save_files: True
  overwrite_files: True  # Don't set this to False without cleaning the folders first
  local_path: ./data
  obj_path: /home/jeff/Documents/target_meshes
  run_target: True
  run_clutter: False
  n_az_samples: 8
  n_el_samples: 8
  num_bounces: 1
  fc: 9600000000.
  nbox_levels: 4


    
# This is for setting up training of target embedding
target_exp_params:
  training_params:
    lr: 0.01
    weight_decay: 0.0001
    scheduler_gamma: 0.99  # Exponential decay
    eta_min: .0000000001  # Cosine annealing
    betas: [ .9, .99 ]
    log_epoch: 1
    save_model: True
    transform_data: False
    warm_start: False
    loss_landscape: False
    max_epochs: 1000
    patience: 10
    angle_samples: 64
  logging_params:
    output_images: False
    model_name: target_encoder
    is_tuning: False
    is_training: True
    log_dir: ./logs
    weights_path: ./model
  dataset_params:
    data_path: ./data/target_tensors
    train_batch_size: 50
    val_batch_size: 50
    num_workers: 0
    pin_memory: False
    split: .7
    single_example: False
    # These values are taken from a sampling of data
    mu: 0.
    var: 1.
    noise_level: 0.
  model_params:
    fft_len: 8192
    range_samples: 2
    latent_dim: 1024
    channel_sz: 1
    label_sz: 25
    nonlinearity: silu
    temperature: 1.
    levels: 2
    # These values are taken from a sampling of data, should be the same as the dataset params
    mu: 0.
    var: 1.

# For training of wavemodel
wave_exp_params:
  training_params:
    lr: 0.000001
    weight_decay: 0.01
    scheduler_gamma: 0.999
    swa_start: .6
    warm_start: False
    betas: [.9, .99]
    log_epoch: 100
    accumulation_steps: 16
    is_tuning: False
    save_model: True
    loss_landscape: False
    max_epochs: 1200
    patience: 500
    distributed: False
  model_params:
    n_ants: 1
    fft_len: 8192
    fs: 2000000000.
    fc: 9600000000.
    encoder_start_channel_sz: 2
    embedding_concatenation_channels: 128
    n_skip_layers: 10
    clutter_target_channels: 128
    flowthrough_channels: 128
    wave_decoder_channels: 128
    target_channels: 128
    waveform_channels: 128
    target_latent_size: 1024
    n_fourier_modes: 18
    bandwidth: 400000000.
  logging_params:
    log_dir: ./logs
    weights_path: ./model
    model_name: wave_model
  dataset_params:
    data_path: ./data
    train_batch_size: 2
    val_batch_size: 2
    num_workers: 0
    pin_memory: False
    split: .7
    single_example: True
    # These values are taken from a sampling of data
    mu: 199926611.3141519
    var: 52500339516.69853
    noise_level: 0.
    collate: True

train_params:
  log_dir: ./logs
